{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 부스트트랩 : 현재 있는 표본에서 추가적으로 표본을 복원 추출하고 각 표본에 대한 통계량을 다시 계산하는 것  ->  모수의 분포를 추정한다.\n",
    "#### 신뢰구간 : 모수가 어느 범위안에 있는지를 확률적으로 보여주는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 앙상블 학습법 2가지: 1.부스팅 2.배깅\n",
    "#### 1. 배깅 : Bootstrap Aggregation의 약자, 배깅은 우선 데이터로부터 부트스트랩을 한다. 그 후 부트스트랩 한 데이터로 모델을 학습하고 학습된 모델의 결과를 집계하여 최종 결과 값을 구한다. 전체 모델에서 예측한 값 중 가장 많은 값을 최종 예측값으로 선택한다 -> 배깅 기법을 활용한 대표적인 예 : 랜덤 포레스트\n",
    "#### 2. 부스팅 : 가중치를 활용하여 약 분류기를 강 분류기로 만드는 방법, 배깅과는 다르게 모델간 팀워크가 이뤄짐. 처음 모델이 예측을 하면 그 예측 결과에 따라 데이터에 가중치가 부여되고 부여된 가중치가 다음 모델에 영향을 줌.\n",
    "#### 배깅과 부스팅의 차이 : 배깅은 병렬로 학습하지만 부스팅은 순차적으로 학습함 -> 속도의 차이가 발생,  부스팅은 배깅에 비해 성능이 좋다 하지만 과적합 문제가 발생할 가능성이 더 높음\n",
    "#### 개별 결정 트리의 낮은 성능이 문제라면 부스팅을 사용하고 과적합 문제라면 배깅사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level-wise 와 Leaf-wise\n",
    "#### Level-wise는 BFS같이 트리를 만들어가고 Leaf-wise는 DFS같이 트리를 만듬\n",
    "#### XGBoost,Catboost 는 Level-wise,  Light GBM은 Leaf-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost는 기본 파라미터의 최적화가 잘 되어있어서 파라미터 튜닝에 신경쓰지 않아도 된다 반면 XGBoost나 Light GBM은 파라미터 튜닝에 민감하다.\n",
    "#### Catboost의 한계 : Sparse한 Matrix는 처리하지 못함, 데이터 대부분이 수치형 변수인 경우, Light GBM보다 학습속도가 느리다(대부분 범주형이라면 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
