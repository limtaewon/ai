{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.columns\n",
    "cols = list(iris.columns)\n",
    "data = cols[:-1]\n",
    "target = cols[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(iris[data],iris[target],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassier 파라미터 \n",
    "### n_neighbors : 근접이웃의 수를 결정하는 파라미터 -> 보통 홀수를 고른다 \n",
    "### n_jobs : 사용할 코어의 갯수 -> -1이면 모든 코어를 사용하는 것\n",
    "\n",
    "\n",
    "#### 예측을 한후 numpy모듈을 이용하여 같으면 1 틀리면 0을 모두 합한 후 평균값을 구하여 정확도를 측정하는 방식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "round(np.mean(y_pred==y_test),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_split = 데이터 그룹의 개수, shuffle = 무작위 선택, random_state = 재현가능 하도록 난수의 초기값을 설정\n",
    "#### KFold(교차검증) 알고리즘을 통하여 10개의 그룹을 선택, 10개그룹중 가장 좋은 성능을 선택하여 정확도 측정을 하였다 그결과 0.1더 높은 정확도를 갖는걸 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(model,X_train,y_train,cv=kf,n_jobs=1,scoring='accuracy')\n",
    "round(np.mean(score),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree(의사결정트리) : 분류와 회귀가 둘다 가능한 모델\n",
    "#### 예/아니요 질문을 이어가며 학습\n",
    "#### 가지치기 기법을 사용하여 과적합을 막을수있다 ex) max_depth\n",
    "#### 결정 트리의 default는 max_depth,min_sample_split 제한이 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "round(np.mean(y_pred==y_test),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "score = cross_val_score(model,X_train,y_train,cv=kf,n_jobs=1,scoring='accuracy')\n",
    "round(np.mean(score),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest(랜덤포레스트) : 여러개의 결정트리로 이루어진 숲\n",
    "#### feature이 많으면 과적합되던 결정트리의 문제를 어느정도 해결 -> 30개의 feature 중 5개만 선택하여 결정트리를 만들고  5개선택해 또다른 결정트리를 만든다 이를 반복하여 결정 트리 하나마다 나온 예측값중 가장 많이 나온 값을 최종 예측값으로 정한다\n",
    "#### 이렇게 의견을 통합하거나 여러가지 결과를 합치는 방식을 앙상블(Ensemble)이라 함\n",
    "#### n_estimator : 랜덤 포레스트 안의 결정 트리 갯수 -> 클수록 좋지만 메모리와 훈련 시간이 그만큼 증가한다.\n",
    "#### max_features : 무작위로 선택할 feature의 갯수 -> boostrap 파라미터가 False이면 비복원 추출하기 때문에 그냥 전체 feature을 사용해 트리를 만들지만 True라면 복원 추출하여 만든다 max_feature의 값이 클수록 특성에 맞게 예측하지만 너무 많으면 과적합 문제가 발생할 수 있다. 반대로 max_feature값이 작다면 랜덤포레스트의 트리들이 달라져 앙상블 하기 어려워지지만 과적합 문제는 해소된다\n",
    "#### bootstrap = True가 Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "round(np.mean(y_pred==y_test),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  RandomForestClassifier(n_estimators=10)\n",
    "score = cross_val_score(model,X_train,y_train,cv=kf,n_jobs=1,scoring='accuracy')\n",
    "round(np.mean(score),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes(나이브 베이즈) : 사전데이터를 기반으로 지도학습\n",
    "##### feature끼리 서로 독립이어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "round(np.mean(y_pred==y_test),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GaussianNB()\n",
    "score = cross_val_score(model,X_train,y_train,cv=kf,n_jobs=1,scoring='accuracy')\n",
    "round(np.mean(score),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
